---
title: "Analysis_IB_ThreatXCongruency"
author: "Ourouk Scylla Lucas Gautier"
date: '2023-11-10'
output:
  html_document: default
  pdf_document: default
  word_document: default
editor_options:
  markdown:
    wrap: 72
---

# Aim of the script

This script is the data analysis for the project
"IB_Threat\*Congruency_Interaction". It use the last extraction of data
collection which means n = 1922 participants who at least began the
study.

OSF link of this project is <https://osf.io/xhc25/> (preregistration is
available at <https://osf.io/8ztcu>)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE)

list.of.packages <- c("dplyr", "tidyverse", "readr", "psych", "ggplot2", "TOSTER", "rsq", "questionr", "esc", "pwr", "effectsize")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
invisible(lapply(list.of.packages, require, character.only = TRUE))

#rm(list=ls()) # clear workspace
rm("list.of.packages", "new.packages")

Initial_data <- read_csv("Transformed_Data/Dataset_Final_ThreatXCongruency.csv")%>%
  select(-"...1") %>%
  mutate(Check_Sound_str = case_when(Check_Sound == -0.5 ~ "No_Sound",
                                     Check_Sound == +0.5 ~ "Sound"))

```

# CodeBook:

## Participant identification

participant: a code name for each participant

ID: An ID number for identify each participant

ID_Yapper: The ID associated to each participant on the CrowdPanel
plateform

## Experimental Conditions

PrimaryTask: Which instructions participants have to perform in terms of
words\
- WhiteFocus = Count bounces of Whites squares\
- BlackFocus = Count bounces of Black squares

PrimaryTask_C: Which instructions participants have to perform in terms
of code\
- -0.5 = White squares bounce counting condition\
- +0.5 = Black squares bounce counting condition

Congruency: Is the unexpected element congruent or incongruent with task
settings in terms of words\
- Incongruent = The unexpected element is incongruent with task settings
(and ppts perform a inattentional blindness task with a low laod : 2
black squares and 2 white squares)\
- Congruent = The unexpected element is congruent with task settings
(and ppts perform a inattentional blindness task with a high laod : 4
black squares and 4 white squares)

Congruency_C: Is the unexpected element congruent or incongruent with
task settings in terms of code\
- -0.5 = The unexpected element is incongruent with task settings (and
ppts perform a inattentional blindness task with a low laod : 2 black
squares and 2 white squares)\
- +0.5 = The unexpected element is congruent with task settings (and
ppts perform a inattentional blindness task with a high laod : 4 black
squares and 4 white squares)

Threat: Which experimental condition participants are assigned at
(Threat VS control), in terms of words\
- Control = Control condition (no sound during the IB task)\
- Threat = Threat condition (an hazard alarm and screams during the IB
task)

Threat_C: Which experimental condition participants are assigned at
(Threat VS control), in terms of code\
- -0.5 = Control condition (a park)\
- +0.5 = Threat condition (a fire)

## Noticing

Noticing_Critic: If participant report having seen something during the
critical trial and report at least one characteristic of unexpected
elements (color, feature, motion)\
- 0 = Non noticer\
- 1 = Noticer who report at least one good feature of the unexpected
element

Noticing_Divided: If participant report having seen something during the
Divided attention trial and report at least one characteristic of
unexpected elements (color, feature, motion)\
- 0 = Non noticer\
- 1 = Noticer who report at least one good feature of the unexpected
element

Noticing_Full: If participant report having seen something during the
Full attention trial and report at least one characteristic of
unexpected elements (color, feature, motion)\
- 0 = Non noticer\
- 1 = Noticer who report at least one good feature of the unexpected
element

## Self Assessment Manikin (SAM)

This scale come from Sainz de Baranda et al., 2022 (SAM_UC3M4Safety)

### Pleasure

PleasureTrial1: Participant rating on the pleasure dimension of the Self
Assessment Manikin after the first trial of the IB task.\
- 1 = Negative feelings\
- 9 = Positive feelings

PleasureCritical: Participant rating on the pleasure dimension of the
Self Assessment Manikin after the critical trial of the IB task.\
- 1 = Negative feelings\
- 9 = Positive feelings

PleasureDivided: Participant rating on the pleasure dimension of the
Self Assessment Manikin after the divided attention trial of the IB
task.\
- 1 = Negative feelings\
- 9 = Positive feelings

### Arousal

ArousalTrial1: Participant rating on the arousal dimension of the Self
Assessment Manikin after the first trial of the IB task.\
- 1 = Calm\
- 9 = High excitation

ArousalCritical: Participant rating on the arousal dimension of the Self
Assessment Manikin after the critical trial of the IB task.\
- 1 = Calm\
- 9 = High excitation

ArousalDivided: Participant rating on the arousal dimension of the Self
Assessment Manikin after the divided attention trial of the IB task.\
- 1 = Calm\
- 9 = High excitation

### Sense of control

SenseControlTrial1: Participant rating on the sense of control dimension
of the Self Assessment Manikin after the first trial of the IB task.\
- 1 = Lack of control\
- 9 = Plenty of control

SenseControlCritical: Participant rating on the sense of control
dimension of the Self Assessment Manikin after the critical trial of the
IB task.\
- 1 = Lack of control\
- 9 = Plenty of control

SenseControlDivided: Participant rating on thesense of control dimension
of the Self Assessment Manikin after the divided attention trial of the
IB task.\
- 1 = Lack of control\
- 9 = Plenty of control

## Bounce-counting performance

Count_Trial1: Number of bounces counted during Trial1

Count_TrialCritic: Number of bounces counted during Critical Trial

Count_TrialDivided: Number of bounces counted during Divided Attention
Trial

Error_Trial1: absolute value of the difference between the real number
of bounce and what the participant reports for Trial 1

Error_TrialCritic: absolute value of the difference between the real
number of bounce and what the participant reports for the Critical Trial

Error_TrialDivided: absolute value of the difference between the real
number of bounce and what the participant reports for the Divided
Attention Trial

## Anxiety scales

### Anxiety scale Pre-test

Participants answers on the anxiety scale at the beginning of the
experiment:\
- Angry_pretest: Participant answer on the angry item from the anxiety
scale ("I currently feel angry"). Responses on a 7-likert scale, from 1
= "Not at all" to 7 = "Extremely".\
- Anxious_pretest: Participant answer on the anxious item from the
anxiety scale ("I currently feel anxious"). Responses on a 7-likert
scale, from 1 = "Not at all" to 7 = "Extremely".\
- Happy_pretest: Participant answer on the happy item from the anxiety
scale ("I currently feel happy"). Responses on a 7-likert scale, from 1
= "Not at all" to 7 = "Extremely".\
- Depressed_pretest: Participant answer on the depressed item from the
anxiety scale ("I'm currently feeling depressed"). Responses on a
7-likert scale, from 1 = "Not at all" to 7 = "Extremely".\
- Tense_pretest: Participant answer on the tense item from the anxiety
scale ("I'm currently feeling tense"). Responses on a 7-likert scale,
from 1 = "Not at all" to 7 = "Extremely".\
- Calm_pretest: Participant answer on the calm item from the anxiety
scale ("I'm currently feeling calm"). Responses on a 7-likert scale,
from 1 = "Not at all" to 7 = "Extremely".\
- Stressed_pretest: Participant answer on the stressed item from the
anxiety scale ("I feel stressed at the moment"). Responses on a 7-likert
scale, from 1 = "Not at all" to 7 = "Extremely".\
- Nervous_pretest: Participant answer on the nervous item from the
anxiety scale ("I'm currently feeling nervous"). Responses on a 7-likert
scale, from 1 = "Not at all" to 7 = "Extremely".

### Anxiety scale Post-test

Participants answers on the anxiety scale after the IB task:\
- Angry: Participant answer on the angry item from the anxiety scale ("I
currently feel angry"). Responses on a 7-likert scale, from 1 = "Not at
all" to 7 = "Extremely".\
- Anxious: Participant answer on the anxious item from the anxiety scale
("I currently feel anxious"). Responses on a 7-likert scale, from 1 =
"Not at all" to 7 = "Extremely".\
- Happy: Participant answer on the happy item from the anxiety scale ("I
currently feel happy"). Responses on a 7-likert scale, from 1 = "Not at
all" to 7 = "Extremely".\
- Depressed: Participant answer on the depressed item from the anxiety
scale ("I'm currently feeling depressed"). Responses on a 7-likert
scale, from 1 = "Not at all" to 7 = "Extremely".\
- Tense: Participant answer on the tense item from the anxiety scale
("I'm currently feeling tense"). Responses on a 7-likert scale, from 1 =
"Not at all" to 7 = "Extremely".\
- Calm: Participant answer on the calm item from the anxiety scale ("I'm
currently feeling calm"). Responses on a 7-likert scale, from 1 = "Not
at all" to 7 = "Extremely".\
- Stressed: Participant answer on the stressed item from the anxiety
scale ("I feel stressed at the moment"). Responses on a 7-likert scale,
from 1 = "Not at all" to 7 = "Extremely".\
- Nervous: Participant answer on the nervous item from the anxiety scale
("I'm currently feeling nervous"). Responses on a 7-likert scale, from 1
= "Not at all" to 7 = "Extremely".

FearScore_pre: A factorial composite score of 4 anxiety items (Tense,
Anxious, Nervous, Stressed) measured at the beginning of the experiment,
using a Bartlett method

FearScore_post: A factorial composite score of 4 anxiety items (Tense,
Anxious, Nervous, Stressed) measured after the experimental induction,
using a Bartlett method

FearScore: The difference between the anxiety factorial score after the
experimental induction and at the beginning of the experiment. Higher
scores reflect an increasing of the anxiety feelings at the
post-measurement in comparison to the pre-measurement (FearScore_post -
FearScore_pre)

FearMean_pre: A composite score of 4 anxiety items (Tense, Anxious,
Tense, Stressed) measured at the beginning of the experiment, using a
mean calculation rather than a factorial score

FearMean_post: A composite score of 4 anxiety items (Tense, Anxious,
Tense, Stressed) after the experimental induction, using a mean
calculation rather than a factorial score

FearMean: The difference between the anxiety mean score after the
experimental induction and at the beginning of the experiment. Higher
scores reflect an increasing of the anxiety feelings at the
post-measurement in comparison to the pre-measurement (FearMean_post -
FearMean_pre)

## Exclusion variables

-   SoundCalibration: Check whether participants correctly heard sounds
    after sound calibration for this experiment. On this question
    participants who respond '1' were redirected to the next
    "Sound_Problem" question, whereas participants who respond '2' or
    '3' went directly to the Inattentional blindness task. Responses on
    this scale were:
    -   NoSound: "I can't hear any sound."
    -   SoftSound: "I can hear the sound but it doesn't seem unpleasant
        to me even though my computer is at maximum volume."
    -   LoudSound: "I can hear the sound at a level that I personally
        find unpleasant."
-   SoundProblem: Participants who reported a problem on the
    "Sound_Calibration" question were redirected to this question in
    order to resolve their sound problem. Then if they responded '2' to
    this item, they were redirected to the "Sound_Calibration"
    questions. In an other way, they were directed to the end of the
    survey, stoping the experiment. Responses on this scale were:
    -   NotSolved: "I have a sound problem: end this experiment."
    -   Solved: "I've solved my sound problem: restart the volume
        calibration."

Check_Sound: Did participants hear any sound during the experiment -
-0.5 = Participants did not hear any sound\
- +0.5 = Participant heard some sounds

Check_Uncomfort: Did participants hear these sound at an uncomfortable
level - -0.5 = Sound level was not uncomfortable\
- +0.5 = Sound level was uncomfortable

Check_Volume: Did participants change the volume of their computer after
the sound calibration - -0.5 = Participants changed their sound level\
- +0.5 = Participants do not change their sound level

Knew: If participants have already done exactly the same task as this
dynamic inattentional blindness task\
- -0.5 = Never made exactly this task\
- +0.5 = Ever made exactly this task

KnewTask: If participants have already done a similar task but not
exactly the same one\
- -0.5 = No task knowledge\
- +0.5 = Task knowledge

KnewIB: If participants knew the inattentional blindness phenomenon
befor to run the experiment\
- -0.5 = No IB knowledge\
- +0.5 = IB knowledge

## Scarcity variables

Subjective_SES: Participant self placement on the McArthur Subjective
SES scale. A 10-point scale - 1 = the lowest social class in the
society - 10 = the highest social class in the society

Pay_Distance: The number of days in which participant will receive a
large amount of money - 1 = participant will be paid in the next day -
31 = participant will be paid in 31 day

Pay_Amount: The amount of money participant will receive the next time
he.she will be paid

Pay_Month: The amount of money participants receive each month (by
adding up their own money and that of their dependents)

Household_Nb: The number of people in the household who live with this
amount of money.

Perceived Economic Scarcity Scale: Participants respond to this scale in
a 7-point Likert Scale from 1 = Not at all, to 7 = Extremely - PESS1: My
income is low compared to others - PESS2: I feel that I have less money
than I need - PESS3: I find it difficult to pay my bills and basic
needs - PESS4: My income is insufficient to live decently - PESS5: I do
not have enough money to cover all my monthly expenses - PESS6: My
limited income and savings make me uncertain about my future - PESS7: I
can't stop thinking about the lack of money - PESS8: I worry about not
having enough money

## Demographics

Age: Participant self rating on age

Gender: Participant gender self identification\
- 1 = Woman self identification\
- 2 = Man self identification\
- 3 = None of these

Town: The town where participants live in

Country: The country participant live in

Studies: The highest level of education achieved by the participants -
No qualification - Secondary schools certificate - CAP (certificate of
professional competence) or BEP (Professional Studies Certificate) -
Baccalaureate - BAC+1 - BAC+2 - BAC+3 - BAC+4 - BAC+5 - BAC+6 - BAC+8 -
more than BAC+8

CSP: Participant occupation - 1 = Farmer, farm operator - 2 =
Craftsperson, shopkeeper, business owner - 3 = Executive and higher
intellectual profession - 4 = Intermediate profession (e.g. teacher,
nurse, social worker, etc.) - 5 = salaried employee - 6 = Manual
worker - 7 = Retired - 8 = Student or in educational training - 9 = Not
in employment or military - 10 = Don't know or can't answer - 11 = Other

CSP_Other: If participant declare another occupation, they can fill in a
blank space to specify it.

Writing_Script_C: Do participant know any script that do not follows the
Latin script (from left to right and up to down script). Another code
for this variable

Writing_Script: Do participant know any script that do not follows the
Latin script (from left to right and up to down script). Another code
for this variable - LatinScript = No - OtherScript = Yes

Writing_Language: If participant declare another script, they can fill
in a blank space to specify it.

Game_Feelings: A blank space in which participants can add any feeling
about this experiment

# Data exclusion

```{r Data exclusion, include = FALSE}

tmp <- df <- Initial_data

# To count the number of NA in each column
# sapply(df, function(x) sum(is.na(x)))

tmp <- tmp %>%
  filter(!is.na(TIME_total)) %>%
  filter(SoundCalibration == "SoftSound" | SoundCalibration == "LoudSound")%>% 
  filter(Check_Sound == +0.5 & Threat == "Threat" | Threat == "Control") %>%
  filter(Knew == -0.5) 


Incong_df <- tmp %>%
  filter(Congruency == "Incongruent") %>%
  mutate(Mad_Incong = case_when(Congruency == "Incongruent" & (Error_TrialCritic >= (median(Error_TrialCritic)+3*mad(Error_TrialCritic))) ~ "Out_Mad", 
                                Congruency == "Incongruent" & (Error_TrialCritic <= (median(Error_TrialCritic)+3*mad(Error_TrialCritic))) ~ "In_Mad"))

Cong_df <- tmp %>%
  filter(Congruency == "Congruent") %>%
  mutate(Mad_Cong = case_when(Congruency == "Congruent" & (Error_TrialCritic >= (median(Error_TrialCritic)+3*mad(Error_TrialCritic))) ~ "Out_Mad", 
                              Congruency == "Congruent" & (Error_TrialCritic <= (median(Error_TrialCritic)+3*mad(Error_TrialCritic))) ~ "In_Mad"))

tmp <- combine(Cong_df, Incong_df) %>%
  mutate(Mad_Cong = ifelse(is.na(Mad_Cong), "Not_Concerned", Mad_Cong),
         Mad_Incong = ifelse(is.na(Mad_Incong), "Not_Concerned", Mad_Incong)) %>%
  filter(Mad_Cong == "In_Mad" | Mad_Incong == "In_Mad")

df <- tmp
rm(tmp)

# sum(is.na(Initial_data$TIME_total))
# table(Initial_data$SoundCalibration)[["NoSound"]]
# table(Initial_data$Threat, Initial_data$Check_Sound_str)["Threat", "No_Sound"]
# table(Initial_data$Knew)[[2]]
# table(Incong_df$Mad_Incong)[["Out_Mad"]]
# table(Cong_df$Mad_Cong)[["Out_Mad"]]


```

Here, we removed some participants according to exclusion rules defined
in our pre-registration. These exclusions means:

-   n = `r sum(is.na(Initial_data$TIME_total))` participants that did
    not complete the study entirely
-   n = `r table(Initial_data$SoundCalibration)[["NoSound"]]`
    participants who report having sound problems during the calibration
    phase
-   n =
    `r table(Initial_data$Threat, Initial_data$Check_Sound_str)["Threat", "No_Sound"]`
    participants who didn't heard sounds during the IB task in the
    threatening condition (however who decided to keep n =
    `r table(Initial_data$Threat, Initial_data$Check_Sound_str)["Control", "Sound"]`
    participants who report having hear some sounds in the control
    condition even if no sound was played)
-   n = `r table(Initial_data$Knew)[[2]]` participants who report having
    previously do exactly the same task
-   n = `r table(Cong_df$Mad_Cong)[["Out_Mad"]]` and n =
    `r table(Incong_df$Mad_Incong)[["Out_Mad"]]` participants whose
    performance on the bounce-counting task is higher than 3 Mad from
    the rest of the sample on the critical trial respectively in the
    congruent and in the incongruent conditions (this bounce-counting
    performance represents the difference between participant's
    performance and the real number of bounces in the critical trial)

Even if a total number of n = `r nrow(Initial_data)` participants
started the experiment, we get only n =
`r nrow(Initial_data)-sum(is.na(Initial_data$TIME_total))` complete
data. In addition, after applying the exclusion rules defined in our
preregistration, the final number of participants to run our analysis is
n = `r nrow(df)`

# Analysis

## Descriptive analysis

`r warning("Add here some descriptive analyses of our variables")`

## Manipulation check

To check the effect of our manipulation on these variables, we applied a
Bonferroni Correction on these 3 analysis according to our
pre-registration (Anxiety feelings, Arousal, Pleasure): alpha = 0.05/3 =
0.017

```{r TOST boundaries, eval=FALSE}

# For Equivalence testing on the effect of our experimental manipulation, use a SESOI of d = .20 seems appropriate given our sample size and the Bonferroni correction:

powerTOSTtwo(alpha = .017, statistical_power = .95, N = 1600)

powerTOSTone(alpha = .017, statistical_power = .95, N = 800)


```

### Anxiety feelings


```{r Anxiety Manip check}

alpha_ManipCheck <- 0.05/3

# Did participants differ in there anxiety level at the beginning of the experiment ? 

Anxiety_Pretest_Check <- lm(FearScore_pre ~ Threat_C, data = df)
summary(Anxiety_Pretest_Check)
rsq.partial(Anxiety_Pretest_Check)

Report_Anxiety_Pretest_Check <- summary(Anxiety_Pretest_Check)


# Equivalence Testing 
Anxiety_Pretest_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(FearScore_pre), list(Mean = mean, SD = sd, Ppt_Nb = length))

Anxiety_Pretest_Check_Tost <- TOSTtwo(m1 = Anxiety_Pretest_Check_Table[[2,2]], sd1 = Anxiety_Pretest_Check_Table[[2,3]], n1 = Anxiety_Pretest_Check_Table[[2,4]],
        m2 = Anxiety_Pretest_Check_Table[[1,2]], sd2 = Anxiety_Pretest_Check_Table[[1,3]], n2 = Anxiety_Pretest_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


# Did participants differ in there anxiety level at the end of the experiment ? 

Anxiety_Posttest_Check <- lm(FearScore_post ~ Threat_C, data = df)
summary(Anxiety_Posttest_Check)
rsq.partial(Anxiety_Posttest_Check)

Report_Anxiety_Posttest_Check <- summary(Anxiety_Posttest_Check)


# Equivalence Testing 
Anxiety_Posttest_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(FearScore_post), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Anxiety_Posttest_Check_Table[[2,2]], sd1 = Anxiety_Posttest_Check_Table[[2,3]], n1 = Anxiety_Posttest_Check_Table[[2,4]],
        m2 = Anxiety_Posttest_Check_Table[[1,2]], sd2 = Anxiety_Posttest_Check_Table[[1,3]], n2 = Anxiety_Posttest_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


# Do the experimental manipulation have an effect on anxiety evolution between pre and post-measurements ?

Anxiety_Evolution_Check <- lm(FearScore ~ Threat_C, data = df)
summary(Anxiety_Evolution_Check)
rsq.partial(Anxiety_Evolution_Check)

Report_Anxiety_Evolution_Check <- summary(Anxiety_Evolution_Check)

# Equivalence Testing 
Anxiety_Evolution_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(FearScore), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Anxiety_Evolution_Check_Table[[2,2]], sd1 = Anxiety_Evolution_Check_Table[[2,3]], n1 = Anxiety_Evolution_Check_Table[[2,4]],
        m2 = Anxiety_Evolution_Check_Table[[1,2]], sd2 = Anxiety_Evolution_Check_Table[[1,3]], n2 = Anxiety_Evolution_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


# Is the task effective for increase the level of anxiety in the threat condition ?

df_Threat <- df %>%
  filter(Threat=="Threat")
  
Anxiety_Check_Threat <- lm(FearScore ~ 1, data = df_Threat)
summary(Anxiety_Check_Threat)

Report_Anxiety_Check_Threat <- summary(Anxiety_Check_Threat)

# The correlation on anxiety between the pre and the post measurement in the Threat condition
Cor_Anxiety_PrePost_Threat <- cor.test(df_Threat$FearScore_pre, df_Threat$FearScore_post) 
print(Cor_Anxiety_PrePost_Threat)

# Equivalence Testing 
Anxiety_Check_Threat_Table_Pre <- df_Threat %>%
    group_by(Threat) %>%
  summarise_at(vars(FearScore_pre), list(Mean = mean, SD = sd, Ppt_Nb = length)) %>%
  mutate(Measurement = "Pretest")
Anxiety_Check_Threat_Table_Post <- df_Threat %>%
    group_by(Threat) %>%
  summarise_at(vars(FearScore_post), list(Mean = mean, SD = sd, Ppt_Nb = length)) %>%
  mutate(Measurement = "PostTest")
Anxiety_Check_Threat_Table <- combine(Anxiety_Check_Threat_Table_Pre, Anxiety_Check_Threat_Table_Post)

TOSTpaired(n = nrow(df_Threat), r12 = Cor_Anxiety_PrePost_Threat[["estimate"]][["cor"]],
           m1 = Anxiety_Check_Threat_Table[[2,2]], sd1 = Anxiety_Check_Threat_Table[[2,3]], 
           m2 = Anxiety_Check_Threat_Table[[1,2]], sd2 = Anxiety_Check_Threat_Table[[1,3]], 
           alpha = alpha_ManipCheck, low_eqbound_dz = -0.20, high_eqbound_dz = 0.20)

# Effect size
Eta_Anxiety_Check_Threat <- esc::eta_squared(esc_mean_sd(
  grp1m = Anxiety_Check_Threat_Table[1,2],
  grp1sd = Anxiety_Check_Threat_Table[1,3],
  grp1n = Anxiety_Check_Threat_Table[1,4],
  grp2m = Anxiety_Check_Threat_Table[2,2],
  grp2sd = Anxiety_Check_Threat_Table[2,3],
  grp2n = Anxiety_Check_Threat_Table[2,4],
  r = Cor_Anxiety_PrePost_Threat[["estimate"]][["cor"]],
  es.type = c("d"))[["es"]][["Mean"]])


# Is the task ineffective for increase the level of anxiety in the control condition ?

df_Control <- df %>%
  filter(Threat=="Control")
  
Anxiety_Check_Control <- lm(FearScore ~ 1, data = df_Control)
summary(Anxiety_Check_Control)

Report_Anxiety_Check_Control <- summary(Anxiety_Check_Control)

# The correlation on anxiety between the pre and the post measurement in the Threat condition
Cor_Anxiety_PrePost_Control <- cor.test(df_Control$FearScore_pre, df_Control$FearScore_post) 
print(Cor_Anxiety_PrePost_Control)

# Equivalence Testing 
Anxiety_Check_Control_Table_Pre <- df_Control %>%
    group_by(Threat) %>%
  summarise_at(vars(FearScore_pre), list(Mean = mean, SD = sd, Ppt_Nb = length)) %>%
  mutate(Measurement = "Pretest")
Anxiety_Check_Control_Table_Post <- df_Control %>%
    group_by(Threat) %>%
  summarise_at(vars(FearScore_post), list(Mean = mean, SD = sd, Ppt_Nb = length)) %>%
  mutate(Measurement = "PostTest")
Anxiety_Check_Control_Table <- combine(Anxiety_Check_Control_Table_Pre, Anxiety_Check_Control_Table_Post)

TOSTpaired(n = nrow(df_Threat), r12 = Cor_Anxiety_PrePost_Control[["estimate"]][["cor"]],
           m1 = Anxiety_Check_Control_Table[[2,2]], sd1 = Anxiety_Check_Control_Table[[2,3]], 
           m2 = Anxiety_Check_Control_Table[[1,2]], sd2 = Anxiety_Check_Control_Table[[1,3]], 
           alpha = alpha_ManipCheck, low_eqbound_dz = -0.20, high_eqbound_dz = 0.20)

# Effect size
Eta_Anxiety_Check_Control <- esc::eta_squared(esc_mean_sd(
  grp1m = Anxiety_Check_Control_Table[1,2],
  grp1sd = Anxiety_Check_Control_Table[1,3],
  grp1n = Anxiety_Check_Control_Table[1,4],
  grp2m = Anxiety_Check_Control_Table[2,2],
  grp2sd = Anxiety_Check_Control_Table[2,3],
  grp2n = Anxiety_Check_Control_Table[2,4],
  r = Cor_Anxiety_PrePost_Control[["estimate"]][["cor"]],
  es.type = c("d"))[["es"]][["Mean"]])

```

To analyse the effect of the experimental manipulation on anxiety
feelings during the experiment, we run multiple linear regression
analyses.

First, we tested whether our two experimental conditions (Threat VS
Control) differed in terms of anxiety feelings at the beginning of the
experiment. This analysis reveals that there is `r ifelse(Report_Anxiety_Pretest_Check$coefficients[2,4]<= 0.05/3 ,"a significant", "a non significant")` effect of experimental condition on anxiety at the beginning of the experiment (*b* =
`r round(Report_Anxiety_Pretest_Check$coefficients[2,1], digits = 2)`,
*F*(`r Report_Anxiety_Pretest_Check$df[1]-1`, `r Report_Anxiety_Pretest_Check$df[2]`) =
`r round(Report_Anxiety_Pretest_Check$coefficients[2,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_Anxiety_Pretest_Check$coefficients[2,4])<= 0.001 ,"< 0.001", ifelse((Report_Anxiety_Pretest_Check$coefficients[2,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_Anxiety_Pretest_Check$coefficients[2,4]), digit = 2))))`).


Some Equivalent testing hypothesis tend to confirm the non-significant results suggesting that the participants do not differ on anxiety level at the beginning of the experiment (*z* = `r ifelse((Anxiety_Pretest_Check_Tost$TOST_t1)<(Anxiety_Pretest_Check_Tost$TOST_t2) ,round(Anxiety_Pretest_Check_Tost$TOST_t1, digit=2), round(Anxiety_Pretest_Check_Tost$TOST_t2, digit=2))`, *p*<sub>corrected at 0.017</sub>  `r ifelse((Anxiety_Pretest_Check_Tost$TOST_p1)>(Anxiety_Pretest_Check_Tost$TOST_p2) & (Anxiety_Pretest_Check_Tost$TOST_p1)<=0.001, "< 0.001", ifelse((Anxiety_Pretest_Check_Tost$TOST_p1)>(Anxiety_Pretest_Check_Tost$TOST_p2) & (Anxiety_Pretest_Check_Tost$TOST_p1)<= 0.01 ,"< 0.01", ifelse((Anxiety_Pretest_Check_Tost$TOST_p1)>(Anxiety_Pretest_Check_Tost$TOST_p2) & (Anxiety_Pretest_Check_Tost$TOST_p1)>= 0.01 , paste0("= ", round((Anxiety_Pretest_Check_Tost$TOST_p1), digit = 3)), ifelse((Anxiety_Pretest_Check_Tost$TOST_p1)<(Anxiety_Pretest_Check_Tost$TOST_p2) & (Anxiety_Pretest_Check_Tost$TOST_p2)<=0.001, "< 0.001", ifelse((Anxiety_Pretest_Check_Tost$TOST_p1)<(Anxiety_Pretest_Check_Tost$TOST_p2) & (Anxiety_Pretest_Check_Tost$TOST_p2)<= 0.01 ,"< 0.01", ifelse((Anxiety_Pretest_Check_Tost$TOST_p1)<(Anxiety_Pretest_Check_Tost$TOST_p2) & (Anxiety_Pretest_Check_Tost$TOST_p2)>= 0.01 , paste0("= ", round((Anxiety_Pretest_Check_Tost$TOST_p2), digit = 2))))))))`, with low equivalence bound = `r Anxiety_Pretest_Check_Tost$low_eqbound_d`, high equivalent bound  = `r Anxiety_Pretest_Check_Tost$high_eqbound_d`, and `r round(100-(5/3*2), digit=1)`% CI [`r round(Anxiety_Pretest_Check_Tost$LL_CI_TOST, digit=3)`, `r round(Anxiety_Pretest_Check_Tost$UL_CI_TOST, digit=3)`]). Participants in the threatening condition (M =
`r round(Anxiety_Pretest_Check_Table[[2,2]], digit = 2)`, SD =
`r round(Anxiety_Pretest_Check_Table[[2,3]], digit = 2)`, n =
`r round(Anxiety_Pretest_Check_Table[[2,4]], digit = 2)`) report as much anxiety as participants the control condition (M =
`r round(Anxiety_Pretest_Check_Table[[1,2]], digit = 2)`, SD =
`r round(Anxiety_Pretest_Check_Table[[1,3]], digit = 2)`, n =
`r round(Anxiety_Pretest_Check_Table[[1,4]], digit = 2)`).

Second, we tested the effect of experimental conditions (Threat VS
Control) on anxiety feelings at the end of the
experiment. This analysis reveals that there is `r ifelse(Report_Anxiety_Posttest_Check$coefficients[2,4]<= 0.05/3 ,"a significant", "a non significant")` effect of experimental condition on anxiety at the end of the experiment (*b* =
`r round(Report_Anxiety_Posttest_Check$coefficients[2,1], digits = 2)`,
*F*(`r Report_Anxiety_Posttest_Check$df[1]-1`, `r Report_Anxiety_Posttest_Check$df[2]`) =
`r round(Report_Anxiety_Posttest_Check$coefficients[2,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_Anxiety_Posttest_Check$coefficients[2,4])<= 0.001 ,"< 0.001", ifelse((Report_Anxiety_Posttest_Check$coefficients[2,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_Anxiety_Posttest_Check$coefficients[2,4]), digit = 2))))`, η² = `r round(rsq.partial(Anxiety_Posttest_Check)[[3]], digit = 3)`). Participants in the threatening condition (M =
`r round(Anxiety_Posttest_Check_Table[[2,2]], digit = 2)`, SD =
`r round(Anxiety_Posttest_Check_Table[[2,3]], digit = 2)`, n =
`r round(Anxiety_Posttest_Check_Table[[2,4]], digit = 2)`) report significantly higher anxiety at the end of the experiment than participants in the control condition (M =
`r round(Anxiety_Posttest_Check_Table[[1,2]], digit = 2)`, SD =
`r round(Anxiety_Posttest_Check_Table[[1,3]], digit = 2)`, n =
`r round(Anxiety_Posttest_Check_Table[[1,4]], digit = 2)`).


Third, we tested whether there is an effect of experimental manipulation between pre and post measurement of anxiety feelings both in the threatening and the control conditions.

In the threatening condition a simple linear regression using the difference between post and pre-measurement reveals that there is `r ifelse(Report_Anxiety_Check_Threat$coefficients[1,4]<= 0.05/3 ,"a significant", "a non significant")` effect of experimental condition on anxiety at the end of the experiment (*b* =
`r round(Report_Anxiety_Check_Threat$coefficients[1,1], digits = 2)`,
*F*(`r Report_Anxiety_Check_Threat$df[1]-1`, `r Report_Anxiety_Check_Threat$df[2]`) =
`r round(Report_Anxiety_Check_Threat$coefficients[1,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_Anxiety_Check_Threat$coefficients[1,4])<= 0.001 ,"< 0.001", ifelse((Report_Anxiety_Check_Threat$coefficients[1,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_Anxiety_Check_Threat$coefficients[1,4]), digit = 2))))`, η² = `r round(Eta_Anxiety_Check_Threat, digit = 3)`). In the threatening condition, participants feel more anxious at the end of the experiment (M =
`r round(Anxiety_Check_Threat_Table[[2,2]], digit = 2)`, SD =
`r round(Anxiety_Check_Threat_Table[[2,3]], digit = 2)`) than at the beginning (M =
`r round(Anxiety_Check_Threat_Table[[1,2]], digit = 2)`, SD =
`r round(Anxiety_Check_Threat_Table[[1,3]], digit = 2)`, n =
`r round(Anxiety_Check_Threat_Table[[1,4]], digit = 2)`).

In the control condition a simple linear regression using the difference between post and pre-measurement reveals that there is `r ifelse(Report_Anxiety_Check_Control$coefficients[1,4]<= 0.05/3 ,"a significant", "a non significant")` effect of experimental condition on anxiety at the end of the experiment (*b* =
`r round(Report_Anxiety_Check_Control$coefficients[1,1], digits = 2)`,
*F*(`r Report_Anxiety_Check_Control$df[1]-1`, `r Report_Anxiety_Check_Control$df[2]`) =
`r round(Report_Anxiety_Check_Control$coefficients[1,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_Anxiety_Check_Control$coefficients[1,4])<= 0.001 ,"< 0.001", ifelse((Report_Anxiety_Check_Control$coefficients[1,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_Anxiety_Check_Control$coefficients[1,4]), digit = 2))))`, η² = `r round(Eta_Anxiety_Check_Control, digit = 3)`). In the control condition, participants feel less anxious at the end of the experiment (M =
`r round(Anxiety_Check_Control_Table[[2,2]], digit = 2)`, SD =
`r round(Anxiety_Check_Control_Table[[2,3]], digit = 2)`) than at the beginning (M =
`r round(Anxiety_Check_Control_Table[[1,2]], digit = 2)`, SD =
`r round(Anxiety_Check_Control_Table[[1,3]], digit = 2)`, n =
`r round(Anxiety_Check_Control_Table[[1,4]], digit = 2)`).

Taken together, a linear regression on the whole sample reveals that the effect of experimental manipulation, increasing anxiety at the end of the experiment is `r ifelse(Report_Anxiety_Evolution_Check$coefficients[2,4]<= 0.05/3 ,"significantly", "not significantly")` higher in the threatening condition than in the control one (*b* =
`r round(Report_Anxiety_Evolution_Check$coefficients[2,1], digits = 2)`,
*F*(`r Report_Anxiety_Evolution_Check$df[1]-1`, `r Report_Anxiety_Evolution_Check$df[2]`) =
`r round(Report_Anxiety_Evolution_Check$coefficients[2,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_Anxiety_Evolution_Check$coefficients[2,4])<= 0.001 ,"< 0.001", ifelse((Report_Anxiety_Evolution_Check$coefficients[2,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_Anxiety_Evolution_Check$coefficients[2,4]), digit = 2))))`, η² = `r round(rsq.partial(Anxiety_Evolution_Check)[[3]], digit = 3)`). 



### Arousal


```{r Arousal Manip check}

# Do participants differ on their arousal level after the critical trial (before being asked about the presence of an unexpected element) ?

Arousal_Critical_Check<- lm(ArousalCritical ~ Threat_C, data = df)
summary(Arousal_Critical_Check)
rsq.partial(Arousal_Critical_Check)

Report_Arousal_Critical_Check <- summary(Arousal_Critical_Check)


# Equivalence Testing 
Arousal_Critical_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(ArousalCritical), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Arousal_Critical_Check_Table[[2,2]], sd1 = Arousal_Critical_Check_Table[[2,3]], n1 = Arousal_Critical_Check_Table[[2,4]],
        m2 = Arousal_Critical_Check_Table[[1,2]], sd2 = Arousal_Critical_Check_Table[[1,3]], n2 = Arousal_Critical_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


```

Then we tested the effect of experimental conditions (Threat VS
Control) on arousal level after the critical trial. This analysis reveals that there is `r ifelse(Report_Arousal_Critical_Check$coefficients[2,4]<= 0.05/3 ,"a significant", "a non significant")` effect of experimental condition on arousal level after the critical trial (*b* =
`r round(Report_Arousal_Critical_Check$coefficients[2,1], digits = 2)`,
*F*(`r Report_Arousal_Critical_Check$df[1]-1`, `r Report_Arousal_Critical_Check$df[2]`) =
`r round(Report_Arousal_Critical_Check$coefficients[2,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_Arousal_Critical_Check$coefficients[2,4])<= 0.001 ,"< 0.001", ifelse((Report_Arousal_Critical_Check$coefficients[2,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_Arousal_Critical_Check$coefficients[2,4]), digit = 2))))`, η² = `r round(rsq.partial(Arousal_Critical_Check)[[3]], digit = 3)`). Participants in the threatening condition (M =
`r round(Arousal_Critical_Check_Table[[2,2]], digit = 2)`, SD =
`r round(Arousal_Critical_Check_Table[[2,3]], digit = 2)`, n =
`r round(Arousal_Critical_Check_Table[[2,4]], digit = 2)`) report significantly higher arousal levels after the critical trial than participants in the control condition (M =
`r round(Arousal_Critical_Check_Table[[1,2]], digit = 2)`, SD =
`r round(Arousal_Critical_Check_Table[[1,3]], digit = 2)`, n =
`r round(Arousal_Critical_Check_Table[[1,4]], digit = 2)`).

```{r Arousal Manip check_Trial1 and Divided, eval=FALSE}

### Trial 1 : Training trial

# Do participants differ on their arousal level after the first trial?

Arousal_Trial1_Check<- lm(ArousalTrial1 ~ Threat_C, data = df)
summary(Arousal_Trial1_Check)
rsq.partial(Arousal_Trial1_Check)

Report_Arousal_Trial1_Check <- summary(Arousal_Trial1_Check)


# Equivalence Testing 
Arousal_Trial1_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(ArousalTrial1), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Arousal_Trial1_Check_Table[[2,2]], sd1 = Arousal_Trial1_Check_Table[[2,3]], n1 = Arousal_Trial1_Check_Table[[2,4]],
        m2 = Arousal_Trial1_Check_Table[[1,2]], sd2 = Arousal_Trial1_Check_Table[[1,3]], n2 = Arousal_Trial1_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)



### Divided Attention trial

# Do participants differ on their arousal level after the first trial?

Arousal_Divided_Check<- lm(ArousalDivided ~ Threat_C, data = df)
summary(Arousal_Divided_Check)
rsq.partial(Arousal_Divided_Check)

Report_Arousal_Divided_Check <- summary(Arousal_Divided_Check)


# Equivalence Testing 
Arousal_Divided_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(ArousalDivided), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Arousal_Divided_Check_Table[[2,2]], sd1 = Arousal_Divided_Check_Table[[2,3]], n1 = Arousal_Divided_Check_Table[[2,4]],
        m2 = Arousal_Divided_Check_Table[[1,2]], sd2 = Arousal_Divided_Check_Table[[1,3]], n2 = Arousal_Divided_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


```

In addition, in an exploratory way, we ran the same analysis on arousal after the training and the divided attention trials. Results are quite the same than after the critical trial.


### Pleasure


```{r Pleasure Manip check}

# Do participants differ on their pleasure level after the critical trial (before being asked about the presence of an unexpected element) ?

Pleasure_Critical_Check<- lm(PleasureCritical ~ Threat_C, data = df)
summary(Pleasure_Critical_Check)
rsq.partial(Pleasure_Critical_Check)

Report_Pleasure_Critical_Check <- summary(Pleasure_Critical_Check)


# Equivalence Testing 
Pleasure_Critical_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(PleasureCritical), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Pleasure_Critical_Check_Table[[2,2]], sd1 = Pleasure_Critical_Check_Table[[2,3]], n1 = Pleasure_Critical_Check_Table[[2,4]],
        m2 = Pleasure_Critical_Check_Table[[1,2]], sd2 = Pleasure_Critical_Check_Table[[1,3]], n2 = Pleasure_Critical_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


```

We also ran the same analysis on the pleasure variable to test the effect of experimental conditions (Threat VS
Control) on negative or positive emotions felt after the critical trial. This analysis reveals that there is `r ifelse(Report_Pleasure_Critical_Check$coefficients[2,4]<= 0.05/3 ,"a significant", "a non significant")` effect of experimental condition on emotions felt after the critical trial (*b* =
`r round(Report_Pleasure_Critical_Check$coefficients[2,1], digits = 2)`,
*F*(`r Report_Pleasure_Critical_Check$df[1]-1`, `r Report_Pleasure_Critical_Check$df[2]`) =
`r round(Report_Pleasure_Critical_Check$coefficients[2,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_Pleasure_Critical_Check$coefficients[2,4])<= 0.001 ,"< 0.001", ifelse((Report_Pleasure_Critical_Check$coefficients[2,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_Pleasure_Critical_Check$coefficients[2,4]), digit = 2))))`, η² = `r round(rsq.partial(Pleasure_Critical_Check)[[3]], digit = 3)`). Participants in the threatening condition (M =
`r round(Pleasure_Critical_Check_Table[[2,2]], digit = 2)`, SD =
`r round(Pleasure_Critical_Check_Table[[2,3]], digit = 2)`, n =
`r round(Pleasure_Critical_Check_Table[[2,4]], digit = 2)`) report significantly lower positive emotions after the critical trial than participants in the control condition (M =
`r round(Pleasure_Critical_Check_Table[[1,2]], digit = 2)`, SD =
`r round(Pleasure_Critical_Check_Table[[1,3]], digit = 2)`, n =
`r round(Pleasure_Critical_Check_Table[[1,4]], digit = 2)`).


```{r Pleasure Manip check_Trial1 and Divided, eval=FALSE}

### Trial 1 : Training trial

# Do participants differ on their Pleasure level after the first trial?

Pleasure_Trial1_Check<- lm(PleasureTrial1 ~ Threat_C, data = df)
summary(Pleasure_Trial1_Check)
rsq.partial(Pleasure_Trial1_Check)

Report_Pleasure_Trial1_Check <- summary(Pleasure_Trial1_Check)


# Equivalence Testing 
Pleasure_Trial1_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(PleasureTrial1), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Pleasure_Trial1_Check_Table[[2,2]], sd1 = Pleasure_Trial1_Check_Table[[2,3]], n1 = Pleasure_Trial1_Check_Table[[2,4]],
        m2 = Pleasure_Trial1_Check_Table[[1,2]], sd2 = Pleasure_Trial1_Check_Table[[1,3]], n2 = Pleasure_Trial1_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)



### Divided Attention trial

# Do participants differ on their Pleasure level after the first trial?

Pleasure_Divided_Check<- lm(PleasureDivided ~ Threat_C, data = df)
summary(Pleasure_Divided_Check)
rsq.partial(Pleasure_Divided_Check)

Report_Pleasure_Divided_Check <- summary(Pleasure_Divided_Check)


# Equivalence Testing 
Pleasure_Divided_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(PleasureDivided), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Pleasure_Divided_Check_Table[[2,2]], sd1 = Pleasure_Divided_Check_Table[[2,3]], n1 = Pleasure_Divided_Check_Table[[2,4]],
        m2 = Pleasure_Divided_Check_Table[[1,2]], sd2 = Pleasure_Divided_Check_Table[[1,3]], n2 = Pleasure_Divided_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


```


In addition, in an exploratory way, we ran the same analysis on emotion felt after the training and the divided attention trials. Results are quite the same than after the critical trial.



### Sense of Control: Exploratory analysis

```{r Sense of Control Manip check}

# Do participants differ on their arousal level after the critical trial (before being asked about the presence of an unexpected element) ?

SenseControl_Critical_Check<- lm(SenseControlCritical ~ Threat_C, data = df)
summary(SenseControl_Critical_Check)
rsq.partial(SenseControl_Critical_Check)

Report_SenseControl_Critical_Check <- summary(SenseControl_Critical_Check)


# Equivalence Testing 
SenseControl_Critical_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(SenseControlCritical), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = SenseControl_Critical_Check_Table[[2,2]], sd1 = SenseControl_Critical_Check_Table[[2,3]], n1 = SenseControl_Critical_Check_Table[[2,4]],
        m2 = SenseControl_Critical_Check_Table[[1,2]], sd2 = SenseControl_Critical_Check_Table[[1,3]], n2 = SenseControl_Critical_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


```

Finally, in an exploratory way, we analysed the effect of experimental conditions (Threat VS Control) on the sense of control participants report after the critical trial. Indeed, Even if we did not have a clear prediction given the influence of our manipulation on this variable, the sense of control is the third dimension measured in the SAM scale we used after each trial. So we decided to run some analysis on it. To analyse its effect, we keep the same alpha threshold we used for the manip check analyses (alpha = `r round(alpha_ManipCheck, digit = 3)`) in order to not so much inflate type I error rate and we find `r ifelse(Report_SenseControl_Critical_Check$coefficients[2,4]<= 0.05/3 ,"a significant", "a non significant")` effect of experimental condition on the sense of control after the critical trial (*b* =
`r round(Report_SenseControl_Critical_Check$coefficients[2,1], digits = 2)`,
*F*(`r Report_SenseControl_Critical_Check$df[1]-1`, `r Report_SenseControl_Critical_Check$df[2]`) =
`r round(Report_SenseControl_Critical_Check$coefficients[2,3], digits = 2)`, *p*<sub>corrected at 0.017</sub> 
`r ifelse((Report_SenseControl_Critical_Check$coefficients[2,4])<= 0.001 ,"< 0.001", ifelse((Report_SenseControl_Critical_Check$coefficients[2,4])<= 0.01 ,"< 0.01", paste0("= ", round((Report_SenseControl_Critical_Check$coefficients[2,4]), digit = 2))))`, η² = `r round(rsq.partial(SenseControl_Critical_Check)[[3]], digit = 3)`). Participants in the threatening condition (M =
`r round(SenseControl_Critical_Check_Table[[2,2]], digit = 2)`, SD =
`r round(SenseControl_Critical_Check_Table[[2,3]], digit = 2)`, n =
`r round(SenseControl_Critical_Check_Table[[2,4]], digit = 2)`) report being significantly less able to control their feelings after the critical trial than participants in the control condition (M =
`r round(SenseControl_Critical_Check_Table[[1,2]], digit = 2)`, SD =
`r round(SenseControl_Critical_Check_Table[[1,3]], digit = 2)`, n =
`r round(SenseControl_Critical_Check_Table[[1,4]], digit = 2)`).


```{r Sense of Control Manip check_Trial1 and Divided, eval=FALSE}

### Trial 1 : Training trial

# Do participants differ on their Sense of Control level after the first trial?

SenseControl_Trial1_Check<- lm(SenseControlTrial1 ~ Threat_C, data = df)
summary(SenseControl_Trial1_Check)
rsq.partial(SenseControl_Trial1_Check)

Report_SenseControl_Trial1_Check <- summary(SenseControl_Trial1_Check)


# Equivalence Testing 
SenseControl_Trial1_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(SenseControlTrial1), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = SenseControl_Trial1_Check_Table[[2,2]], sd1 = SenseControl_Trial1_Check_Table[[2,3]], n1 = SenseControl_Trial1_Check_Table[[2,4]],
        m2 = SenseControl_Trial1_Check_Table[[1,2]], sd2 = SenseControl_Trial1_Check_Table[[1,3]], n2 = SenseControl_Trial1_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)



### Divided Attention trial

# Do participants differ on their Sense of Control level after the first trial?

SenseControl_Divided_Check<- lm(SenseControlDivided ~ Threat_C, data = df)
summary(SenseControl_Divided_Check)
rsq.partial(SenseControl_Divided_Check)

Report_SenseControl_Divided_Check <- summary(SenseControl_Divided_Check)


# Equivalence Testing 
SenseControl_Divided_Check_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(SenseControlDivided), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = SenseControl_Divided_Check_Table[[2,2]], sd1 = SenseControl_Divided_Check_Table[[2,3]], n1 = SenseControl_Divided_Check_Table[[2,4]],
        m2 = SenseControl_Divided_Check_Table[[1,2]], sd2 = SenseControl_Divided_Check_Table[[1,3]], n2 = SenseControl_Divided_Check_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.20, high_eqbound_d = 0.20)


```

In addition, in an exploratory way, we ran the same analysis on Sense of Control after the training and the divided attention trials. Results are quite the same than after the critical trial.




## Threat effect on noticing

```{r Congruency dataframes, include=FALSE}

rm(Cong_df, Incong_df)

# Build the Congruent data frame
df_Cong <- df %>%
  filter(Congruency == "Congruent")

# Build the Incongruent data frame
df_Incong <- df %>%
  filter(Congruency == "Incongruent")

```

### Threat: Congruent unexpected element

```{r Effect of threat on noticing_Congruent}

# Do the threatening manipulation have an effect on noticing rate for the congruent unexpected element ?

Threat_Cong <- glm(Noticing_Critic ~ Threat_C , family ="binomial", data = df_Cong)
summary(Threat_Cong)
odds.ratio(Threat_Cong) # Calculate ODD RATIO of the model


# With Covariates
Threat_Cong_Cov <- glm(Noticing_Critic ~ Threat_C + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df_Cong)
summary(Threat_Cong_Cov)
odds.ratio(Threat_Cong_Cov) # Calculate ODD RATIO of the model
rsq.partial(Threat_Cong_Cov) # Calculate an equivalent of R²p of the model

Coef_Threat_Cong_Cov <- summary(Threat_Cong_Cov)[[12]]

# Equivalence testing
Table_Threat_Cong_Nb <- table(df_Cong$Threat, dnn = "Threatening_Condition")

Table_NoticingCritic_Threat_Cong_Nb <- table(df_Cong$Noticing_Critic, df_Cong$Threat, dnn = c("Congruent_element_Noticing", "Threatening_Condition"))

Table_NoticingCritic_Threat_Cong_Prop <- round(prop.table(table(df_Cong$Noticing_Critic, df_Cong$Threat, dnn = c("Congruent_element_Noticing", "Threatening_Condition")), margin = 2), digits = 2)

Critic_Cong_TOST <- TOSTtwo.prop(prop1 = Table_NoticingCritic_Threat_Cong_Prop[2, 2], n1 = Table_Threat_Cong_Nb[2],
                                 prop2 = Table_NoticingCritic_Threat_Cong_Prop[2, 1], n2 = Table_Threat_Cong_Nb[1],
                                 alpha = 0.05, low_eqbound = -.20, high_eqbound = .20)


# Calculate the effect size in Cohen's h: 
h_Critic_Threat_Cong <-  round(effectsize::cohens_h(Table_NoticingCritic_Threat_Cong_Nb, ci = 0.95, alternative = "two.sided", log = FALSE), digits = 2)

# Calculate the effect size in Cohen's d: 
round(esc::cohens_d(or =odds.ratio(Threat_Cong_Cov)[[2,1]]), digit =2)
  
# This Equivalence bounds of .20 for the effect of threat on congruent noticing means approximately a Cohen's d of:
h_Critic_Threat_Cong_Bounds <-  round(ES.h(Table_NoticingCritic_Threat_Cong_Prop[2, 2]-((.20 - (Table_NoticingCritic_Threat_Cong_Prop[2, 1]-Table_NoticingCritic_Threat_Cong_Prop[2, 2]))/2), Table_NoticingCritic_Threat_Cong_Prop[2, 1]+((.20 - (Table_NoticingCritic_Threat_Cong_Prop[2, 1]-Table_NoticingCritic_Threat_Cong_Prop[2, 2]))/2)), digit = 2)

```

On the noticing of the congruent unexpected element, a logistic regression revealed that the threatening manipulation of threat `r ifelse(Coef_Threat_Cong_Cov[[2,4]]<= 0.05 ,"have a significant", "do not have a significant")` effect on noticing (*b* = `r round(Threat_Cong_Cov$coefficients[["Threat_C"]], digits = 2)`, *z*(`r Threat_Cong_Cov[["df.residual"]]`) = `r round(Coef_Threat_Cong_Cov[[2,3]], digits = 2)`, *p* `r ifelse((Coef_Threat_Cong_Cov[[2,4]])<= 0.001 ,"< 0.001", ifelse((Coef_Threat_Cong_Cov[[2,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_Threat_Cong_Cov[[2,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(Threat_Cong_Cov)[[2,1]], digit = 2)`, CI 95% [`r round(odds.ratio(Threat_Cong_Cov)[[2,2]], digit = 2)`, `r round(odds.ratio(Threat_Cong_Cov)[[2,3]], digit = 2)`]) and that this effect is neither made significant after controlling for the color on which participants have to focus on for the primary task (*b* = `r round(Threat_Cong_Cov$coefficients[["PrimaryTask_C"]], digits = 2)`, *z*(`r Threat_Cong_Cov[["df.residual"]]`) = `r round(Coef_Threat_Cong_Cov[[3,3]], digits = 2)`, *p* `r ifelse((Coef_Threat_Cong_Cov[[3,4]])<= 0.001 ,"< 0.001", ifelse((Coef_Threat_Cong_Cov[[3,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_Threat_Cong_Cov[[3,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(Threat_Cong_Cov)[[3,1]], digit = 2)`, CI 95% [`r round(odds.ratio(Threat_Cong_Cov)[[3,2]], digit = 2)`, `r round(odds.ratio(Threat_Cong_Cov)[[3,3]], digit = 2)`]), nor after controlling the number of error on the bounce-counting task for the critical trial even if this last variable was significant predicting noticing of the congruent unexpected element (*b* = `r round(Threat_Cong_Cov$coefficients[["Error_TrialCritic"]], digits = 2)`, *z*(`r Threat_Cong_Cov[["df.residual"]]`) = `r round(Coef_Threat_Cong_Cov[[4,3]], digits = 2)`, *p* `r ifelse((Coef_Threat_Cong_Cov[[4,4]])<= 0.001 ,"< 0.001", ifelse((Coef_Threat_Cong_Cov[[4,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_Threat_Cong_Cov[[4,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(Threat_Cong_Cov)[[4,1]], digit = 2)`, CI 95% [`r round(odds.ratio(Threat_Cong_Cov)[[4,2]], digit = 2)`, `r round(odds.ratio(Threat_Cong_Cov)[[4,3]], digit = 2)`]). This last result show that the more participants make error on the bounce counting task, the less they notice the congruent unexpected element.

Some equivalent testing hypothesis tend to confirm the non-significant results suggesting that the threatening manipulation do not have an effect on the noticing of the congruent unexpected element (*z* = `r ifelse((Critic_Cong_TOST$TOST_z1)<(Critic_Cong_TOST$TOST_z2) ,round(Critic_Cong_TOST$TOST_z1, digit=2), round(Critic_Cong_TOST$TOST_z2, digit=2))`, *p* `r ifelse((Critic_Cong_TOST$TOST_p1)>(Critic_Cong_TOST$TOST_p2) & (Critic_Cong_TOST$TOST_p1)<=0.001, "< 0.001", ifelse((Critic_Cong_TOST$TOST_p1)>(Critic_Cong_TOST$TOST_p2) & (Critic_Cong_TOST$TOST_p1)<= 0.01 ,"< 0.01", ifelse((Critic_Cong_TOST$TOST_p1)>(Critic_Cong_TOST$TOST_p2) & (Critic_Cong_TOST$TOST_p1)>= 0.01 , paste0("= ", round((Critic_Cong_TOST$TOST_p1), digit = 2)), ifelse((Critic_Cong_TOST$TOST_p1)<(Critic_Cong_TOST$TOST_p2) & (Critic_Cong_TOST$TOST_p2)<=0.001, "< 0.001", ifelse((Critic_Cong_TOST$TOST_p1)<(Critic_Cong_TOST$TOST_p2) & (Critic_Cong_TOST$TOST_p2)<= 0.01 ,"< 0.01", ifelse((Critic_Cong_TOST$TOST_p1)<(Critic_Cong_TOST$TOST_p2) & (Critic_Cong_TOST$TOST_p2)>= 0.01 , paste0("= ", round((Critic_Cong_TOST$TOST_p2), digit = 2))))))))`, with low equivalence bound = `r Critic_Cong_TOST$low_eqbound`, high equivalent bound  = `r Critic_Cong_TOST$high_eqbound`, and 90% CI [`r round(Critic_Cong_TOST$LL_CI_TOST, digit=3)`, `r round(Critic_Cong_TOST$UL_CI_TOST, digit=3)`]). Given the effect size boundaries we used (20% proportion differences around `r (Table_NoticingCritic_Threat_Cong_Prop[[2,1]]+ Table_NoticingCritic_Threat_Cong_Prop[[2,2]])/2*100`% noticing), we can conclude that the difference in noticing on the congruent element between the control and the threatening conditions (respectively `r Table_NoticingCritic_Threat_Cong_Prop[[2,1]]*100`% and `r Table_NoticingCritic_Threat_Cong_Prop[[2,2]]*100`% noticings, which represent a Cohen's *h* of `r h_Critic_Threat_Cong[[1]]`, `r h_Critic_Threat_Cong[[2]]*100`% CI [`r h_Critic_Threat_Cong[[3]]`, `r h_Critic_Threat_Cong[[4]]`]) is statistically lower than a Cohen's *h* = `r h_Critic_Threat_Cong_Bounds`, corresponding to a small to medium effect size.

Warning: rajouter le fait que le test d'équivalence est également significatif via l'utilisation d'un SESOI de .10 même s'il s'agit d'un SESOI défini à Posteriori ?


### Threat: Incongruent unexpected element

```{r Effect of threat on noticing_Incongruent}

# Do the threatening manipulation have an effect on noticing rate for the incongruent unexpected element ?

Threat_Incong <- glm(Noticing_Critic ~ Threat_C , family ="binomial", data = df_Incong)
summary(Threat_Incong)
odds.ratio(Threat_Incong) # Calculate ODD RATIO of the model


# With Covariates
Threat_Incong_Cov <- glm(Noticing_Critic ~ Threat_C + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df_Incong)
summary(Threat_Incong_Cov)
odds.ratio(Threat_Incong_Cov) # Calculate ODD RATIO of the model
rsq.partial(Threat_Incong_Cov) # Calculate an equivalent of R²p of the model

Coef_Threat_Incong_Cov <- summary(Threat_Incong_Cov)[[12]]


# Equivalence testing
Table_Threat_Incong_Nb <- table(df_Incong$Threat, dnn = "Threatening_Condition")

Table_NoticingCritic_Threat_Incong_Nb <- table(df_Incong$Noticing_Critic, df_Incong$Threat, dnn = c("Incongruent_element_Noticing", "Threatening_Condition"))

Table_NoticingCritic_Threat_Incong_Prop <- round(prop.table(table(df_Incong$Noticing_Critic, df_Incong$Threat, dnn = c("Incongruent_element_Noticing", "Threatening_Condition")), margin = 2), digits = 2)

Critic_Incong_TOST <- TOSTtwo.prop(prop1 = Table_NoticingCritic_Threat_Incong_Prop[2, 2], n1 = Table_Threat_Incong_Nb[2],
        prop2 = Table_NoticingCritic_Threat_Incong_Prop[2, 1], n2 = Table_Threat_Incong_Nb[1],
        alpha = 0.05, low_eqbound = -.20, high_eqbound = .20)


# Calculate the effect size in Cohen's h: 
h_Critic_Threat_Incong <-  round(cohens_h(Table_NoticingCritic_Threat_Incong_Nb, ci = 0.95, alternative = "two.sided", log = FALSE), digits = 2)

# Calculate the effect size in Cohen's d: 
round(esc::cohens_d(or =odds.ratio(Threat_Incong_Cov)[[2,1]]), digit =2)

# This Equivalence bounds of .20 for the effect of threat on incongruent noticing means approximately a Cohen's d of:
h_Critic_Threat_Incong_Bounds <-  round(ES.h(Table_NoticingCritic_Threat_Incong_Prop[2, 2]-((.20 - (Table_NoticingCritic_Threat_Incong_Prop[2, 1]-Table_NoticingCritic_Threat_Incong_Prop[2, 2]))/2), Table_NoticingCritic_Threat_Incong_Prop[2, 1]+((.20 - (Table_NoticingCritic_Threat_Incong_Prop[2, 1]-Table_NoticingCritic_Threat_Incong_Prop[2, 2]))/2)), digit = 2)

```

On the noticing of the Incongruent unexpected element, a second logistic regression revealed that the threatening manipulation of threat `r ifelse(Coef_Threat_Incong_Cov[[2,4]]<= 0.05 ,"have a significant", "do not have a significant")` effect on noticing (*b* = `r round(Threat_Incong_Cov$coefficients[["Threat_C"]], digits = 2)`, *z*(`r Threat_Incong_Cov[["df.residual"]]`) = `r round(Coef_Threat_Incong_Cov[[2,3]], digits = 2)`, *p* `r ifelse((Coef_Threat_Incong_Cov[[2,4]])<= 0.001 ,"< 0.001", ifelse((Coef_Threat_Incong_Cov[[2,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_Threat_Incong_Cov[[2,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(Threat_Incong_Cov)[[2,1]], digit = 2)`, CI 95% [`r round(odds.ratio(Threat_Incong_Cov)[[2,2]], digit = 2)`, `r round(odds.ratio(Threat_Incong_Cov)[[2,3]], digit = 2)`]) and that this effect is hold significant after controlling for the color on which participants have to focus on for the primary task (*b* = `r round(Threat_Incong_Cov$coefficients[["PrimaryTask_C"]], digits = 2)`, *z*(`r Threat_Incong_Cov[["df.residual"]]`) = `r round(Coef_Threat_Incong_Cov[[3,3]], digits = 2)`, *p* `r ifelse((Coef_Threat_Incong_Cov[[3,4]])<= 0.001 ,"< 0.001", ifelse((Coef_Threat_Incong_Cov[[3,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_Threat_Incong_Cov[[3,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(Threat_Incong_Cov)[[3,1]], digit = 2)`, CI 95% [`r round(odds.ratio(Threat_Incong_Cov)[[3,2]], digit = 2)`, `r round(odds.ratio(Threat_Incong_Cov)[[3,3]], digit = 2)`]), and participant's errors on the bounce-counting task (*b* = `r round(Threat_Incong_Cov$coefficients[["Error_TrialCritic"]], digits = 2)`, *z*(`r Threat_Incong_Cov[["df.residual"]]`) = `r round(Coef_Threat_Incong_Cov[[4,3]], digits = 2)`, *p* `r ifelse((Coef_Threat_Incong_Cov[[4,4]])<= 0.001 ,"< 0.001", ifelse((Coef_Threat_Incong_Cov[[4,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_Threat_Incong_Cov[[4,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(Threat_Incong_Cov)[[4,1]], digit = 2)`, CI 95% [`r round(odds.ratio(Threat_Incong_Cov)[[4,2]], digit = 2)`, `r round(odds.ratio(Threat_Incong_Cov)[[4,3]], digit = 2)`]). Contrary to the analysis on the congruent unexpected element, this effect of bounce counting performance on noticing is only marginally significant but is in the same way than before: the more participants make error on the bounce counting task, the less they tend to notice the incongruent unexpected element. Moreover, according to our hypothesis pre-registrated, the first result suggest that a threatening situation decrease the noticing of an incongruent element (`r Table_NoticingCritic_Threat_Incong_Prop[[2,2]]*100`% notincing) in comparison to a more neutral situation (`r Table_NoticingCritic_Threat_Incong_Prop[[2,1]]*100`% noticing).

Finally, equivalence testing analysis add some information to this results since it allows us to conclude that this effect is significantly lower than a difference of 20% in proportion between the two conditions (*z* = `r ifelse((Critic_Incong_TOST$TOST_z1)<(Critic_Incong_TOST$TOST_z2) ,round(Critic_Incong_TOST$TOST_z1, digit=2), round(Critic_Incong_TOST$TOST_z2, digit=2))`, *p* `r ifelse((Critic_Incong_TOST$TOST_p1)>(Critic_Incong_TOST$TOST_p2) & (Critic_Incong_TOST$TOST_p1)<=0.001, "< 0.001", ifelse((Critic_Incong_TOST$TOST_p1)>(Critic_Incong_TOST$TOST_p2) & (Critic_Incong_TOST$TOST_p1)<= 0.01 ,"< 0.01", ifelse((Critic_Incong_TOST$TOST_p1)>(Critic_Incong_TOST$TOST_p2) & (Critic_Incong_TOST$TOST_p1)>= 0.01 , paste0("= ", round((Critic_Incong_TOST$TOST_p1), digit = 2)), ifelse((Critic_Incong_TOST$TOST_p1)<(Critic_Incong_TOST$TOST_p2) & (Critic_Incong_TOST$TOST_p2)<=0.001, "< 0.001", ifelse((Critic_Incong_TOST$TOST_p1)<(Critic_Incong_TOST$TOST_p2) & (Critic_Incong_TOST$TOST_p2)<= 0.01 ,"< 0.01", ifelse((Critic_Incong_TOST$TOST_p1)<(Critic_Incong_TOST$TOST_p2) & (Critic_Incong_TOST$TOST_p2)>= 0.01 , paste0("= ", round((Critic_Incong_TOST$TOST_p2), digit = 2))))))))`, with low equivalence bound = `r Critic_Incong_TOST$low_eqbound`, high equivalent bound  = `r Critic_Incong_TOST$high_eqbound`, and 90% CI [`r round(Critic_Incong_TOST$LL_CI_TOST, digit=3)`, `r round(Critic_Incong_TOST$UL_CI_TOST, digit=3)`]).
So, given the effect size boundaries we used (20% proportion differences around `r (Table_NoticingCritic_Threat_Incong_Prop[[2,1]]+ Table_NoticingCritic_Threat_Incong_Prop[[2,2]])/2*100`% noticing), we can conclude that the difference in noticing on the incongruent element between the control and the threatening conditions (respectively `r Table_NoticingCritic_Threat_Incong_Prop[[2,1]]*100`% and `r Table_NoticingCritic_Threat_Incong_Prop[[2,2]]*100`% noticings, which represent a Cohen's *h* of `r h_Critic_Threat_Incong[[1]]`, `r h_Critic_Threat_Incong[[2]]*100`% CI [`r h_Critic_Threat_Incong[[3]]`, `r h_Critic_Threat_Incong[[4]]`]) is statistically lower than a Cohen's *h* = `r h_Critic_Threat_Incong_Bounds`, corresponding to a medium effect size.

Warning: But why Cohen's *h* = ES.h(.25, .12) = `r h_Critic_Threat_Incong` is not equal to a converted Cohen's *d* from (OR = .44): Cohen's *d* = cohens_d(or =0.44248) = `r round(esc::cohens_d(or =0.44248), digit =2)`

To understand Cohen's *h*, see: <https://www.statskingdom.com/effect-size-calculator.html> and <https://en.wikipedia.org/wiki/Cohen%27s_h>



### Threat: Threat X Congruency interaction

According to our preregistration, we will used a p-value threshold of
aplha = 0.01 on this analysis.

```{r Threat X Congruency interaction}

# Do the threatening manipulation interact with congruency to predict noticing rate using the whole dataframe (with participants who belong both to the congruent and the incongruent conditions) ?

ThreatXCongruency <- glm(Noticing_Critic ~ Threat_C * Congruency_C , family ="binomial", data = df)
summary(ThreatXCongruency)
odds.ratio(ThreatXCongruency, level = 0.99) # Calculate ODD RATIO of the model
rsq.partial(ThreatXCongruency) # Calculate an equivalent of R²p of the model


# With Covariates
ThreatXCongruency_Cov <- glm(Noticing_Critic ~ Threat_C * Congruency_C + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df)
summary(ThreatXCongruency_Cov)
odds.ratio(ThreatXCongruency_Cov, level = 0.99) # Calculate ODD RATIO of the model
rsq.partial(ThreatXCongruency_Cov) # Calculate an equivalent of R²p of the model

Coef_ThreatXCongruency_Cov <- summary(ThreatXCongruency_Cov)[[12]]


## Noticing rate

## Threat manipulation
Table_Threat_Nb <- table(df$Threat, dnn = "Threatening_Condition")

Table_NoticingCritic_Threat_Nb <- table(df$Noticing_Critic, df$Threat, dnn = c("Unexpected_element_Noticing", "Threatening_Condition"))

Table_NoticingCritic_Threat_Prop <- round(prop.table(table(df$Noticing_Critic, df$Threat, dnn = c("Unexpected_element_Noticing", "Threatening_Condition")), margin = 2), digits = 2)

## Congruency manipulation
Table_Congruency_Nb <- table(df$Congruency, dnn = "Congruency_Condition")

Table_NoticingCritic_Congruency_Nb <- table(df$Noticing_Critic, df$Congruency, dnn = c("Unexpected_element_Noticing", "Congruency_Condition"))

Table_NoticingCritic_Congruency_Prop <- round(prop.table(table(df$Noticing_Critic, df$Congruency, dnn = c("Unexpected_element_Noticing", "Congruency_Condition")), margin = 2), digits = 2)


```

On the interaction between congruency and experimental manipulation, according to our pre-registration we used an alpha threshold of alpha = 0.01 in order not to inflate so much the type I error rate and results reveal `r ifelse(Coef_ThreatXCongruency_Cov[[6,4]]<= 0.01 ,"a significant", "a non significant")` interaction on noticing during the critical trial 

(*b* = `r round(ThreatXCongruency_Cov$coefficients[["Threat_C:Congruency_C"]], digits = 2)`, *z*(`r ThreatXCongruency_Cov[["df.residual"]]`) = `r round(Coef_ThreatXCongruency_Cov[[6,3]], digits = 2)`, *p*<sub>corrected at 0.01</sub> `r ifelse((Coef_ThreatXCongruency_Cov[[6,4]])<= 0.001 ,"< 0.001", ifelse((Coef_ThreatXCongruency_Cov[[6,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_ThreatXCongruency_Cov[[6,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(ThreatXCongruency_Cov)[[6,1]], digit = 2)`, CI 99% [`r round(odds.ratio(ThreatXCongruency_Cov)[[6,2]], digit = 2)`, `r round(odds.ratio(ThreatXCongruency_Cov)[[6,3]], digit = 2)`]).
Moreover, this interaction is maintained after controlling for the effect of the color participants have to focus on for the bounce counting task

(*b* = `r round(ThreatXCongruency_Cov$coefficients[["PrimaryTask_C"]], digits = 2)`, *z*(`r ThreatXCongruency_Cov[["df.residual"]]`) = `r round(Coef_ThreatXCongruency_Cov[[4,3]], digits = 2)`, *p*<sub>corrected at 0.01</sub> `r ifelse((Coef_ThreatXCongruency_Cov[[4,4]])<= 0.001 ,"< 0.001", ifelse((Coef_ThreatXCongruency_Cov[[4,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_ThreatXCongruency_Cov[[4,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(ThreatXCongruency_Cov)[[4,1]], digit = 2)`, CI 99% [`r round(odds.ratio(ThreatXCongruency_Cov)[[4,2]], digit = 2)`, `r round(odds.ratio(ThreatXCongruency_Cov)[[4,3]], digit = 2)`]) and their performance on this task for which a better performance is marginally associated with more noticing of the unexpected element using the 0.01 alpha threshold 

(*b* = `r round(ThreatXCongruency_Cov$coefficients[["Error_TrialCritic"]], digits = 2)`, *z*(`r ThreatXCongruency_Cov[["df.residual"]]`) = `r round(Coef_ThreatXCongruency_Cov[[5,3]], digits = 2)`, *p*<sub>corrected at 0.01</sub> `r ifelse((Coef_ThreatXCongruency_Cov[[5,4]])<= 0.001 ,"< 0.001", ifelse((Coef_ThreatXCongruency_Cov[[5,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_ThreatXCongruency_Cov[[5,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(ThreatXCongruency_Cov)[[5,1]], digit = 2)`, CI 99% [`r round(odds.ratio(ThreatXCongruency_Cov)[[5,2]], digit = 2)`, `r round(odds.ratio(ThreatXCongruency_Cov)[[5,3]], digit = 2)`]).

In terms of main effect, we find `r ifelse(Coef_ThreatXCongruency_Cov[[2,4]]<= 0.01 ,"a significant", "a non significant")` effect of experimental manipulation suggesting that participants notice less the unexpected element in the threatening condition (`r Table_NoticingCritic_Threat_Prop[[2,2]]*100`% noticing) in comparison to the control one (`r Table_NoticingCritic_Threat_Prop[[2,1]]*100`% noticing, 


*b* = `r round(ThreatXCongruency_Cov$coefficients[["Threat_C"]], digits = 2)`, *z*(`r ThreatXCongruency_Cov[["df.residual"]]`) = `r round(Coef_ThreatXCongruency_Cov[[2,3]], digits = 2)`, *p*<sub>corrected at 0.01</sub> `r ifelse((Coef_ThreatXCongruency_Cov[[2,4]])<= 0.001 ,"< 0.001", ifelse((Coef_ThreatXCongruency_Cov[[2,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_ThreatXCongruency_Cov[[2,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(ThreatXCongruency_Cov)[[2,1]], digit = 2)`, CI 99% [`r round(odds.ratio(ThreatXCongruency_Cov)[[2,2]], digit = 2)`, `r round(odds.ratio(ThreatXCongruency_Cov)[[2,3]], digit = 2)`]). In addition, we find `r ifelse(Coef_ThreatXCongruency_Cov[[3,4]]<= 0.01 ,"a significant", "a non significant")` effect of congruency on noticing revealing that participant better notice the unexpected element when it is congruent with task settings (`r Table_NoticingCritic_Congruency_Prop[[2,2]]*100`% noticing) than when it is incongruent (`r Table_NoticingCritic_Congruency_Prop[[2,1]]*100`% noticing, 

*b* = `r round(ThreatXCongruency_Cov$coefficients[["Congruency_C"]], digits = 2)`, *z*(`r ThreatXCongruency_Cov[["df.residual"]]`) = `r round(Coef_ThreatXCongruency_Cov[[3,3]], digits = 2)`, *p*<sub>corrected at 0.01</sub> `r ifelse((Coef_ThreatXCongruency_Cov[[3,4]])<= 0.001 ,"< 0.001", ifelse((Coef_ThreatXCongruency_Cov[[3,4]])<= 0.01 ,"< 0.01", paste0("= ", round((Coef_ThreatXCongruency_Cov[[3,4]]), digit = 2))))`, *OR* = `r round(odds.ratio(ThreatXCongruency_Cov)[[3,1]], digit = 2)`, CI 99% [`r round(odds.ratio(ThreatXCongruency_Cov)[[3,2]], digit = 2)`, `r round(odds.ratio(ThreatXCongruency_Cov)[[3,3]], digit = 2)`]). However, it is hard to conclude on this last effect since there is a confound on this variable which is no controlled for. Indeed, the congruency variable do not only manipulate the congruency of the unexpected element but also the perceptual load of each trial (by the number of targets and distractors in the main task) which often have an impact on noticing. So, even if we can't conclude that the mere congruency have an impact on noticing (and specifically given our standardised effect size *b* = `r round(ThreatXCongruency_Cov$coefficients[["Congruency_C"]], digits = 2)`) since we don't control for perceptual load, results from the litterature (and from own previous experiments) sytematically show that the noticing of a congruent unexpected element is higher than a incongruent one, controlling for the number of stimuli whitin the task. 

rajouter le fait que c'est pas si génant paske la tache est plus dure mais on identifie quand même l'élément inattendu congruent....


Blablabla, what this interaction means exactely?



###############################################"

## Anxiety effect on notincing

### Anxiety/Arousal: correlations

```{r Anxiety/Arousal Correlations}

# Is there a correlation between Anxiety (the pre and post-measurement difference) and Arousal levels (measured after the critical trial) ?

Cor_Anxiety_Arousal <- cor.test(df$FearScore, df$ArousalCritical) 
print(Cor_Anxiety_Arousal)

alpha_AnxietyArousal <- ifelse(Cor_Anxiety_Arousal[["p.value"]]<= 0.05 & Cor_Anxiety_Arousal[["estimate"]][["cor"]]<= 0.5, 0.05, (0.05/2))

# We can also use the correlation above which do not use the evolution score of anxiety but only the post test score that better reflect the Arousal measurement:
# Cor_Anxiety_Arousal_Post <- cor.test(df$FearScore_post, df$ArousalCritical) 
#print(Cor_Anxiety_Arousal_Post)

```

Since the correlation between the anxiety score (difference between pre
and post measurements) and the arousal level after the critical trial is
`r ifelse(Cor_Anxiety_Arousal[["p.value"]]<= 0.05 ,"significant", "not significant")`,
and the link between these variable is
`r ifelse(Cor_Anxiety_Arousal[["estimate"]][["cor"]]<= 0.5 ,"lesser", "higher")`
than *r* = 0.50 (*r* =
`r round(Cor_Anxiety_Arousal[["estimate"]][["cor"]], digit = 2)`,
t(`r Cor_Anxiety_Arousal[["parameter"]][["df"]]`) =
`r round(Cor_Anxiety_Arousal[["statistic"]][["t"]], digit=2)`, *p*
`r ifelse((Cor_Anxiety_Arousal[["p.value"]])<= 0.001 ,"< 0.001", ifelse((Cor_Anxiety_Arousal[["p.value"]])<= 0.01 ,"< 0.01", paste0("= ", round((Cor_Anxiety_Arousal[["p.value"]]), digit = 2))))`,
CI 95% [`r round(Cor_Anxiety_Arousal[["conf.int"]][[1]], digit = 2)`,
`r round(Cor_Anxiety_Arousal[["conf.int"]][[2]], digit = 2)`]),
according to our preregistration we will use a significance alpha
threshold of alpha =
`r ifelse(Cor_Anxiety_Arousal[["estimate"]][["cor"]]<= 0.5 ,"0.05 (without applying any correction)", ".05/2 = .025 (to correct for multiple analysis)")`
for the next analyses: the effect of Anxiety and Arousal on noticing
rates.

### Anxiety/Arousal: Congruent unexpected element

```{r Effect of Anxiety/Arousal on noticing_Congruent}

# Anxiety level

# Do the anxiety level have an effect on noticing rate for the congruent unexpected element ?

Anxiety_Cong <- glm(Noticing_Critic ~ FearScore , family ="binomial", data = df_Cong)
summary(Anxiety_Cong)
odds.ratio(Anxiety_Cong) # Calculate ODD RATIO of the model


# With Covariates
Anxiety_Cong_Cov <- glm(Noticing_Critic ~ FearScore + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df_Cong)
summary(Anxiety_Cong_Cov)
odds.ratio(Anxiety_Cong_Cov) # Calculate ODD RATIO of the model
rsq.partial(Anxiety_Cong_Cov) # Calculate an equivalent of R²p of the model


# Equivalence testing
Cor_Noticing_Anxiety_Cong <- cor.test(df_Cong$Noticing_Critic, df_Cong$FearScore) 
TOSTr(r = Cor_Noticing_Anxiety_Cong[["estimate"]][["cor"]], n = nrow(df_Cong), alpha = alpha_AnxietyArousal, low_eqbound_r = -.20, high_eqbound_r = .20)

##############################################################

# Arousal level

# Do the arousal level have an effect on noticing rate for the congruent unexpected element ?

Arousal_Cong <- glm(Noticing_Critic ~ ArousalCritical , family ="binomial", data = df_Cong)
summary(Arousal_Cong)
odds.ratio(Arousal_Cong) # Calculate ODD RATIO of the model


# With Covariates
Arousal_Cong_Cov <- glm(Noticing_Critic ~ ArousalCritical + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df_Cong)
summary(Arousal_Cong_Cov)
odds.ratio(Arousal_Cong_Cov) # Calculate ODD RATIO of the model
rsq.partial(Arousal_Cong_Cov) # Calculate an equivalent of R²p of the model


# Equivalence testing
Cor_Noticing_Arousal_Cong <- cor.test(df_Cong$Noticing_Critic, df_Cong$ArousalCritical) 
TOSTr(r = Cor_Noticing_Arousal_Cong[["estimate"]][["cor"]], n = nrow(df_Cong), alpha = alpha_AnxietyArousal, low_eqbound_r = -.20, high_eqbound_r = .20)

```

### Anxiety/Arousal: Incongruent unexpected element

```{r Effect of Anxiety/Arousal on noticing_Incongruent}

# Anxiety level

# Do the anxiety level have an effect on noticing rate for the incongruent unexpected element ?

Anxiety_Incong <- glm(Noticing_Critic ~ FearScore , family ="binomial", data = df_Incong)
summary(Anxiety_Incong)
odds.ratio(Anxiety_Incong) # Calculate ODD RATIO of the model


# With Covariates
Anxiety_Incong_Cov <- glm(Noticing_Critic ~ FearScore + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df_Incong)
summary(Anxiety_Incong_Cov)
odds.ratio(Anxiety_Incong_Cov) # Calculate ODD RATIO of the model
rsq.partial(Anxiety_Incong_Cov) # Calculate an equivalent of R²p of the model


# Equivalence testing
Cor_Noticing_Anxiety_Incong <- cor.test(df_Incong$Noticing_Critic, df_Incong$FearScore) 
TOSTr(r = Cor_Noticing_Anxiety_Incong[["estimate"]][["cor"]], n = nrow(df_Incong), alpha = alpha_AnxietyArousal, low_eqbound_r = -.20, high_eqbound_r = .20)

##############################################################

# Arousal level

# Do the arousal level have an effect on noticing rate for the incongruent unexpected element ?


Arousal_Incong <- glm(Noticing_Critic ~ ArousalCritical , family ="binomial", data = df_Incong)
summary(Arousal_Incong)
odds.ratio(Arousal_Incong) # Calculate ODD RATIO of the model


# With Covariates
Arousal_Incong_Cov <- glm(Noticing_Critic ~ ArousalCritical + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df_Incong)
summary(Arousal_Incong_Cov)
odds.ratio(Arousal_Incong_Cov) # Calculate ODD RATIO of the model
rsq.partial(Arousal_Incong_Cov) # Calculate an equivalent of R²p of the model


# Equivalence testing
Cor_Noticing_Arousal_Incong <- cor.test(df_Incong$Noticing_Critic, df_Incong$ArousalCritical) 
TOSTr(r = Cor_Noticing_Arousal_Incong[["estimate"]][["cor"]], n = nrow(df_Incong), alpha = alpha_AnxietyArousal, low_eqbound_r = -.20, high_eqbound_r = .20)

```

### Anxiety/Arousal: Threat X Congruency interaction

According to our preregistration, we will use a p-value threshold of
aplha = 0.01 on this analysis

```{r Anxiety/Arousal X Congruency interaction}

# Anxiety level

# Do the anxiety level interact with congruency to predict noticing rate using the whole dataframe (with participants who belong both to the congruent and the incongruent conditions) ?

AnxietyXCongruency <- glm(Noticing_Critic ~ FearScore * Congruency_C , family ="binomial", data = df)
summary(AnxietyXCongruency)
odds.ratio(AnxietyXCongruency) # Calculate ODD RATIO of the model
rsq.partial(AnxietyXCongruency) # Calculate an equivalent of R²p of the model


# With Covariates
AnxietyXCongruency_Cov <- glm(Noticing_Critic ~ FearScore * Congruency_C + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df)
summary(AnxietyXCongruency_Cov)
odds.ratio(AnxietyXCongruency_Cov) # Calculate ODD RATIO of the model
rsq.partial(AnxietyXCongruency_Cov) # Calculate an equivalent of R²p of the model

############################################################################

# Arousal level

# Do the arousal level interact with congruency to predict noticing rate using the whole dataframe (with participants who belong both to the congruent and the incongruent conditions) ?

ArousalXCongruency <- glm(Noticing_Critic ~ ArousalCritical * Congruency_C , family ="binomial", data = df)
summary(ArousalXCongruency)
odds.ratio(ArousalXCongruency) # Calculate ODD RATIO of the model
rsq.partial(ArousalXCongruency) # Calculate an equivalent of R²p of the model


# With Covariates
ArousalXCongruency_Cov <- glm(Noticing_Critic ~ ArousalCritical * Congruency_C + PrimaryTask_C + Error_TrialCritic , family ="binomial", data = df)
summary(ArousalXCongruency_Cov)
odds.ratio(ArousalXCongruency_Cov) # Calculate ODD RATIO of the model
rsq.partial(ArousalXCongruency_Cov) # Calculate an equivalent of R²p of the model

```

## Interaction Effect on noticing (Threat X Anxiety)

### Interaction: Congruent unexpected element

```{r Interaction Threat X Anxiety/Arousal_Congruent}

# Anxiety level

# Do the threatening manipulation interact with anxiety level to predict noticing rate for the congruent unexpected element ?

Interact_ThreatXAnxiety_Cong <- glm(Noticing_Critic ~ Threat_C * FearScore, family ="binomial", data = df_Cong)
summary(Interact_ThreatXAnxiety_Cong)
odds.ratio(Interact_ThreatXAnxiety_Cong) # Calculate ODD RATIO of the model
rsq.partial(Interact_ThreatXAnxiety_Cong) # Calculate an equivalent of R²p of the model


############################################################################

# Arousal level

# Do the threatening manipulation interact with arousal level to predict noticing rate for the congruent unexpected element ?

Interact_ThreatXArousal_Cong <- glm(Noticing_Critic ~ Threat_C *ArousalCritical, family ="binomial", data = df_Cong)
summary(Interact_ThreatXArousal_Cong)
odds.ratio(Interact_ThreatXArousal_Cong) # Calculate ODD RATIO of the model
rsq.partial(Interact_ThreatXArousal_Cong) # Calculate an equivalent of R²p of the model


```

### Interaction: Incongruent unexpected element

```{r Interaction Threat X Anxiety/Arousal_Incongruent}

# Anxiety level

# Do the threatening manipulation interact with Anxiety level to predict noticing rate for the incongruent unexpected element ?

Interact_ThreatXAnxiety_Incong <- glm(Noticing_Critic ~ Threat_C * FearScore, family ="binomial", data = df_Incong)
summary(Interact_ThreatXAnxiety_Incong)
odds.ratio(Interact_ThreatXAnxiety_Incong) # Calculate ODD RATIO of the model
rsq.partial(Interact_ThreatXAnxiety_Incong) # Calculate an equivalent of R²p of the model


############################################################################

# Arousal level

# Do the threatening manipulation interact with arousal level to predict noticing rate for the incongruent unexpected element ?

Interact_ThreatXArousal_Incong <- glm(Noticing_Critic ~ Threat_C *ArousalCritical, family ="binomial", data = df_Incong)
summary(Interact_ThreatXArousal_Incong)
odds.ratio(Interact_ThreatXArousal_Incong) # Calculate ODD RATIO of the model
rsq.partial(Interact_ThreatXArousal_Incong) # Calculate an equivalent of R²p of the model

```

### Interaction: Equivalence testing

#### TOST Anxiety: Congruent unexpected element

```{r TOST Noticing~Anxiety_Congruent}

# Control Condition

# Is the effect between Anxiety level and noticing in the control condition smaller than a SESOI for the congruent unexpected element (r = .20) ? (We pre-registered it will)

df_Control_Cong <- df %>%
  filter(Threat == "Control" & Congruency == "Congruent")

Cor_CtrlCond_Noticing_Anxiety_Cong <- cor.test(df_Control_Cong$Noticing_Critic, df_Control_Cong$FearScore) 
TOSTr(r = Cor_CtrlCond_Noticing_Anxiety_Cong[["estimate"]][["cor"]], n = nrow(df_Control_Cong), alpha = .025, low_eqbound_r = -.20, high_eqbound_r = .20)


# Threat Condition

# Is the effect between Anxiety level and noticing in the threatening condition smaller than a SESOI for the congruent unexpected element (r = .20) ? (We pre-registered it will not)

df_Threat_Cong <- df %>%
  filter(Threat == "Threat" & Congruency == "Congruent")

Cor_ThreatCond_Noticing_Anxiety_Cong <- cor.test(df_Threat_Cong$Noticing_Critic, df_Threat_Cong$FearScore) 
TOSTr(r = Cor_ThreatCond_Noticing_Anxiety_Cong[["estimate"]][["cor"]], n = nrow(df_Threat_Cong), alpha = .025, low_eqbound_r = -.20, high_eqbound_r = .20)

```

#### TOST Anxiety: Incongruent unexpected element

```{r TOST Noticing~Anxiety_Incongruent}

# Control Condition

# Is the effect between Anxiety level and noticing in the control condition smaller than a SESOI for the incongruent unexpected element (r = .20) ? (We pre-registered it will)

df_Control_Incong <- df %>%
  filter(Threat == "Control" & Congruency == "Incongruent")

Cor_CtrlCond_Noticing_Anxiety_Incong <- cor.test(df_Control_Incong$Noticing_Critic, df_Control_Incong$FearScore) 
TOSTr(r = Cor_CtrlCond_Noticing_Anxiety_Incong[["estimate"]][["cor"]], n = nrow(df_Control_Incong), alpha = .025, low_eqbound_r = -.20, high_eqbound_r = .20)


# Threat Condition

# Is the effect between Anxiety level and noticing in the threatening condition smaller than a SESOI for the incongruent unexpected element (r = .20) ? (We pre-registered it will not)

df_Threat_Incong <- df %>%
  filter(Threat == "Threat" & Congruency == "Incongruent")

Cor_ThreatCond_Noticing_Anxiety_Incong <- cor.test(df_Threat_Incong$Noticing_Critic, df_Threat_Incong$FearScore) 
TOSTr(r = Cor_ThreatCond_Noticing_Anxiety_Incong[["estimate"]][["cor"]], n = nrow(df_Threat_Incong), alpha = .025, low_eqbound_r = -.20, high_eqbound_r = .20)

```

#### TOST Exploratory: Arousal

In order not to inflate type I error rates, we will use a alpha
threshold of 0.01 for these exploratory equivalence analyses

##### TOST Arousal: Congruent unexpected element

```{r TOST Noticing~Arousal_Congruent, eval=FALSE}

# Control Condition

# Is the effect between Anxiety level and noticing in the threatening condition smaller than a SESOI for the incongruent unexpected element (r = .20) ?

df_Control_Cong <- df %>%
  filter(Threat == "Control" & Congruency == "Congruent")

Cor_CtrlCond_Noticing_Arousal_Cong <- cor.test(df_Control_Cong$Noticing_Critic, df_Control_Cong$ArousalCritical) 
TOSTr(r = Cor_CtrlCond_Noticing_Arousal_Cong[["estimate"]][["cor"]], n = nrow(df_Control_Cong), alpha = .01, low_eqbound_r = -.20, high_eqbound_r = .20)


# Threat Condition

# Is the effect between Anxiety level and noticing in the threatening condition smaller than a SESOI for the congruent unexpected element (r = .20) ?

df_Threat_Cong <- df %>%
  filter(Threat == "Threat" & Congruency == "Congruent")

Cor_ThreatCond_Noticing_Arousal_Cong <- cor.test(df_Threat_Cong$Noticing_Critic, df_Threat_Cong$ArousalCritical) 
TOSTr(r = Cor_ThreatCond_Noticing_Arousal_Cong[["estimate"]][["cor"]], n = nrow(df_Threat_Cong), alpha = .01, low_eqbound_r = -.20, high_eqbound_r = .20)

```

##### TOST Arousal: Incongruent unexpected element

```{r TOST Noticing~Arousal_Incongruent, eval=FALSE}

# Control Condition

# Is the effect between Anxiety level and noticing in the control condition smaller than a SESOI for the incongruent unexpected element (r = .20) ?

df_Control_Incong <- df %>%
  filter(Threat == "Control" & Congruency == "Incongruent")

Cor_CtrlCond_Noticing_Arousal_Incong <- cor.test(df_Control_Incong$Noticing_Critic, df_Control_Incong$ArousalCritical) 
TOSTr(r = Cor_CtrlCond_Noticing_Arousal_Incong[["estimate"]][["cor"]], n = nrow(df_Control_Incong), alpha = .01, low_eqbound_r = -.20, high_eqbound_r = .20)


# Threat Condition

# Is the effect between Anxiety level and noticing in the threatening condition smaller than a SESOI for the incongruent unexpected element (r = .20) ?

df_Threat_Incong <- df %>%
  filter(Threat == "Threat" & Congruency == "Incongruent")

Cor_ThreatCond_Noticing_Arousal_Incong <- cor.test(df_Threat_Incong$Noticing_Critic, df_Threat_Incong$ArousalCritical) 
TOSTr(r = Cor_ThreatCond_Noticing_Arousal_Incong[["estimate"]][["cor"]], n = nrow(df_Threat_Incong), alpha = .01, low_eqbound_r = -.20, high_eqbound_r = .20)

```

## Bounce counting performance

### Counting: Congruent unexpected element

```{r Effect of Threat on counting performance_Congruent}

# Do the threatening manipulation have an impact on bounce counting performance in the high load task?

Counting_Cong <- lm(Error_TrialCritic ~ Threat_C, data = df_Cong)
summary(Counting_Cong)
rsq.partial(Counting_Cong)

# With Covariates
Counting_Cong_Cov <- lm(Error_TrialCritic ~ Threat_C + PrimaryTask_C, data = df_Cong)
summary(Counting_Cong_Cov)
rsq.partial(Counting_Cong_Cov) 


# Equivalence Testing 
Counting_Cong_Table <- df %>%
    group_by(Threat) %>%
    summarise_at(vars(Error_TrialCritic), list(Mean = mean, SD = sd, Ppt_Nb = length))

TOSTtwo(m1 = Counting_Cong_Table[[2,2]], sd1 = Counting_Cong_Table[[2,3]], n1 = Counting_Cong_Table[[2,4]],
        m2 = Counting_Cong_Table[[1,2]], sd2 = Counting_Cong_Table[[1,3]], n2 = Counting_Cong_Table[[1,4]],
        alpha = alpha_ManipCheck, low_eqbound_d = -0.30, high_eqbound_d = 0.30)

# Equivalence testing
#TOSTtwo(alpha = .05, low_eqbound_d = -0.30, high_eqbound_d = 0.30)

```

### Counting: Incongruent unexpected element

```{r Effect of Threat on counting performance_Incongruent}

# Do the threatening manipulation have an impact on bounce counting performance in the low load task?

Counting_Incong <- lm(Error_TrialCritic ~ Threat_C, data = df_Incong)
summary(Counting_Incong)
rsq.partial(Counting_Incong)

# With Covariates
Counting_Incong_Cov <- lm(Error_TrialCritic ~ Threat_C + PrimaryTask_C, data = df_Incong)
summary(Counting_Incong_Cov)
rsq.partial(Counting_Incong_Cov) 

# Equivalence testing
#TOSTtwo(alpha = .05, low_eqbound_d = -0.30, high_eqbound_d = 0.30)

```

### Counting: Threat X Congruency Interaction

According to other analyses, we will use a p-value threshold of aplha =
0.01 on this analysis

```{r Interaction Threat * Congruency on counting performance}

# Do the threatening manipulation interact with congruency to predict the bounce counting performance using the whole dataframe (with participants who belong both to the congruent and the incongruent conditions) ?

Counting_Interact <- lm(Error_TrialCritic ~ Threat_C * Congruency_C, data = df)
summary(Counting_Interact)
rsq.partial(Counting_Interact)

# With Covariates
Counting_Interact_Cov <- lm(Error_TrialCritic ~ Threat_C * Congruency_C + PrimaryTask_C, data = df)
summary(Counting_Interact_Cov)
rsq.partial(Counting_Interact_Cov) 

```

## Other Inattentional Blindness trials

We plan to analyse the effect of experimental conditions on noticing
rates in the Divided attention and the full attention trials but we will
do run these analysis in an exploratory way since we do not have precise
hypothesis on these trials except for the same hypothesis we expect on
the critical trial. However, we do not have planned these analyses in
this pre-registration script.
Ptète que je pourrai faire du mixte pour tester les trois essais en même temps ?

## Multivers analysis

We plan to use this type of analysis to test the robustness of some
effects according to multiple exclusion criterion, however, we do not
have planned these analyses in this pre-registration script..

## Random forest analysis

We plan to maybe run conditional random forest analysis to analyse in an
exploratory way the variables that best predict noticing rates on the
several inattentional blindness trials. However, we do not have planned
these analyses in this pre-registration script..


## Effect of primary task on noticing

```{r Primary task effect}

# Effect of noticing on primary task errors
Primary <- lm(Error_TrialCritic ~ Noticing_Critic + Congruency_C , data = df)
summary(Primary)

# Effect of noticing on primary task errors
Primary_Noticing <- glm(Noticing_Critic  ~ Error_TrialCritic + Congruency_C , family = "binomial", data = df)
summary(Primary_Noticing)


```


